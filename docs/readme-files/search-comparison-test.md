# Search Comparison Test: MCP vs Manual vs Drag-and-Drop

This document presents **actual benchmark results** comparing three approaches:
1. **MCP semantic search** - AI uses `search_code` tool
2. **Manual Grep+Read** - AI searches with Grep, then reads files
3. **Drag-and-Drop** - User manually attaches files to AI chat

> **Latest Results (Full Codebase)**: MCP achieves **40-43x token efficiency** vs grep on a 249-file codebase.
> See the [full codebase analysis report](../tests/reports/full-codebase-analysis-2025-12-13.md) for comprehensive benchmarks.

> **Note**: These results are from automated benchmarks. Run them yourself with:
> ```bash
> npm run test:configs           # Config matrix tests (synthetic fixture)
> npm run test:configs:full      # Full codebase tests
> npx vitest run tests/benchmarks/search-comparison.test.ts  # Benchmark tests
> ```

## Codebase Under Test

| Metric | Value |
|--------|-------|
| **Total files indexed** | 238 |
| **Total chunks created** | 970 |
| **Codebase** | Search MCP (this project) |

## Test Queries

| # | Query | Type | Description |
|---|-------|------|-------------|
| 1 | "how does file watching work" | Conceptual | Understanding a system concept |
| 2 | "error handling patterns" | Pattern | Finding code patterns |
| 3 | "LanceDB vector search" | Technical | Specific technology usage |
| 4 | "security vulnerabilities" | Broad | Security-related content |
| 5 | "configuration options" | Documentation | Finding config docs |

---

## Test Results

### Query 1: "how does file watching work"

#### MCP Results
- **Tool calls**: 1
- **Results returned**: 8 (after deduplication from 10 raw)
- **Characters**: 36,129
- **Measured tokens**: 9,032
- **Search time**: 16-19ms (after model warmup)
- **Relevance**: HIGH - Direct matches for file watcher implementation, debouncing, event handling

#### Manual Results (Grep + Read)
- **Tool calls**: Would need 25+ (1 Grep + reading matched files)
- **Files matched**: 25
- **Total matches**: 607 occurrences
- **Characters**: 420,897
- **Measured tokens**: 105,224
- **Relevance**: MEDIUM - Need to manually select which files to read

#### Drag-and-Drop Results
- **User effort**: HIGH - Must know which files to attach
- **Files to attach**: fileWatcher.ts, integrity.ts, strategyOrchestrator.ts
- **Characters**: 74,553
- **Measured tokens**: 18,638
- **Relevance**: HIGH (if correct files chosen) / LOW (if wrong files chosen)
- **Problem**: User must already know where file watching is implemented

---

### Query 2: "error handling patterns"

#### MCP Results
- **Tool calls**: 1
- **Results returned**: 9 (after deduplication from 10 raw)
- **Characters**: 33,540
- **Measured tokens**: 8,385
- **Search time**: 18-19ms
- **Relevance**: HIGH - Found MCPError class, wrapError(), error factories, test patterns

#### Manual Results (Grep + Read)
- **Tool calls**: Would need 45+ (1 Grep + reading matched files)
- **Files matched**: 45
- **Total matches**: 2,197 occurrences
- **Characters**: 700,909
- **Measured tokens**: 175,227
- **Relevance**: LOW - Too many matches, high noise ratio

#### Drag-and-Drop Results
- **User effort**: HIGH - Error handling is scattered across codebase
- **Files to attach**: errors/index.ts (primary error definitions)
- **Characters**: 11,056
- **Measured tokens**: 2,764
- **Relevance**: MEDIUM - Only covers error definitions, misses usage patterns
- **Problem**: Error patterns exist in 45 files - impractical to attach all

---

### Query 3: "LanceDB vector search"

#### MCP Results
- **Tool calls**: 1
- **Results returned**: 8 (after deduplication from 10 raw)
- **Characters**: 29,418
- **Measured tokens**: 7,355
- **Search time**: 24ms
- **Relevance**: HIGH - Found LanceDB store implementation, search functions, tests

#### Manual Results (Grep + Read)
- **Tool calls**: Would need 44+ (1 Grep + reading matched files)
- **Files matched**: 44
- **Total matches**: 1,098 occurrences
- **Characters**: 610,503
- **Measured tokens**: 152,626
- **Relevance**: MEDIUM - Keyword matches but includes package.json, changelogs, etc.

#### Drag-and-Drop Results
- **User effort**: MEDIUM - LanceDB files are more concentrated
- **Files to attach**: lancedb.ts, docsLancedb.ts, searchCode.ts, searchDocs.ts
- **Characters**: 66,877
- **Measured tokens**: 16,719
- **Relevance**: HIGH (if user knows the architecture)
- **Problem**: User must understand codebase structure first

---

### Query 4: "security vulnerabilities"

#### MCP Results
- **Tool calls**: 1
- **Results returned**: 9 (after deduplication from 10 raw)
- **Characters**: 22,794
- **Measured tokens**: 5,699
- **Search time**: 16ms
- **Relevance**: HIGH - Found security sections, path security, symlink protection

#### Manual Results (Grep + Read)
- **Tool calls**: Would need 34+ (1 Grep + reading matched files)
- **Files matched**: 34
- **Total matches**: 376 occurrences
- **Characters**: 546,067
- **Measured tokens**: 136,517
- **Relevance**: MEDIUM - Many false positives (e.g., comments mentioning "security")

#### Drag-and-Drop Results
- **User effort**: VERY HIGH - Security is cross-cutting concern
- **Files to attach**: secureFileAccess.ts, paths.ts, indexPolicy.ts
- **Characters**: 63,793
- **Measured tokens**: 15,948
- **Relevance**: LOW - Security concerns are distributed throughout codebase
- **Problem**: User cannot know all files with security implications

---

### Query 5: "configuration options"

#### MCP Results
- **Tool calls**: 1
- **Results returned**: 6 (after deduplication from 10 raw)
- **Characters**: 19,999
- **Measured tokens**: 5,000
- **Search time**: 19ms
- **Relevance**: HIGH - Found ConfigManager, config schema, default options, documentation

#### Manual Results (Grep + Read)
- **Tool calls**: Would need 43+ (1 Grep + reading matched files)
- **Files matched**: 43
- **Total matches**: 843 occurrences
- **Characters**: 633,495
- **Measured tokens**: 158,374
- **Relevance**: MEDIUM - Matches spread across many files

#### Drag-and-Drop Results
- **User effort**: MEDIUM - Config is relatively concentrated
- **Files to attach**: config.ts, metadata.ts
- **Characters**: 37,008
- **Measured tokens**: 9,252
- **Relevance**: HIGH (if user knows config locations)
- **Problem**: May miss config options in tool files, schema definitions

---

## Summary Table (Measured Data)

| Query | MCP Tokens | Grep Tokens | D&D Tokens | MCP vs Grep | MCP vs D&D |
|-------|------------|-------------|------------|-------------|------------|
| 1. File watching | 9,224 | 108,015 | 18,638 | **11.7x** | **2.0x** |
| 2. Error handling | 7,628 | 192,191 | 2,764 | **25.2x** | 0.4x* |
| 3. LanceDB search | 9,506 | 174,552 | 19,852 | **18.4x** | **2.1x** |
| 4. Security | 5,104 | 143,376 | 16,268 | **28.1x** | **3.2x** |
| 5. Configuration | 7,591 | 177,003 | 10,445 | **23.3x** | **1.4x** |
| **TOTAL** | **39,053** | **795,137** | **67,967** | **20.4x** | **1.7x** |

*Query 2 D&D only includes errors/index.ts - incomplete coverage

---

## Deduplication Effectiveness

The search results now include automatic deduplication of same-file chunks:

| Query | Raw Results | After Dedup | Reduction |
|-------|-------------|-------------|-----------|
| File watching | 10 | 8 | -20% |
| Error handling | 10 | 9 | -10% |
| LanceDB search | 10 | 8 | -20% |
| Security | 10 | 9 | -10% |
| Configuration | 10 | 6 | **-40%** |

---

## Key Takeaways

### 1. Token Efficiency (Measured)
- **MCP is ~20.4x more token-efficient than Grep** across all query types
- **MCP is ~1.7x more efficient than optimal D&D** (but D&D requires expertise)
- MCP returns focused chunks (~7,800 tokens avg) vs full files
- Grep would require reading 25-49 files per query (impractical)

### 2. Search Speed (Measured)
- **MCP search completes in 14-17ms** (after model warmup)
- First query takes ~400ms due to embedding model initialization
- Subsequent queries are near-instant

### 3. Relevance Quality
- **MCP consistently achieves HIGH relevance** - semantic search understands intent
- Manual Grep returns MEDIUM-LOW relevance - keyword matching produces noise
- D&D relevance varies: HIGH if correct files selected, LOW otherwise

### 4. Scalability
- **MCP scales consistently** - always 6-10 deduplicated results, ~7,800 tokens regardless of codebase
- Grep scales with codebase - more files = more tokens to read
- D&D requires increasing expertise as codebase grows

### 5. The D&D Paradox
- D&D can be **more efficient** than MCP for specific queries (Query 2: 2,764 vs 7,628)
- BUT requires user to **already know the answer** (which files to attach)
- This is a chicken-and-egg problem: to know what to attach, you need to search first

### 6. When Each Approach Wins

| Approach | Best For |
|----------|----------|
| **MCP** | Exploratory queries, broad topics, understanding unfamiliar code |
| **Grep** | Exact string matching, finding specific variable/function names |
| **D&D** | Explaining a specific file you're already looking at |

---

## Methodology

### Benchmark Implementation
All benchmarks are automated and reproducible. See `tests/benchmarks/search-comparison.test.ts`.

### Token Estimation
- 1 token â‰ˆ 4 characters (standard approximation)
- Tokens = Characters / 4

### MCP Measurement
- Actual `search_code` tool calls with `top_k=10`
- Results are automatically deduplicated (adjacent/overlapping chunks merged)
- Measured characters from returned chunk text
- Search time from actual execution (excluding first query warmup)

### Grep Measurement
- Simulated grep with relevant regex patterns per query
- Counted actual matched files in codebase
- Calculated total characters if ALL matched files were read

### Drag-and-Drop Measurement
- Identified minimum files a knowledgeable user would attach
- Calculated actual file sizes
- Assumes best-case scenario (user knows exactly what to attach)

---

## Codebase Statistics

At time of benchmark (2025-12-11):
- **Total files indexed**: 238
- **Total chunks created**: 970
- **Total characters (grep scope)**: ~795,137 tokens worth
- **Average MCP result**: ~7,800 tokens per query

---

## Optimizations Applied

The following optimizations are now active in MCP search:

1. **Same-file deduplication** (SMCP-063): Merges adjacent/overlapping chunks from the same file
2. **Whitespace trimming** (SMCP-064): Removes leading/trailing blank lines from chunks
3. **Compact output format** (SMCP-065): Available via `compact: true` parameter
4. **Code-aware chunking** (SMCP-066): Available via `chunkingStrategy: 'code-aware'` config

---

## Conclusion

**MCP semantic search is the clear winner for exploratory code searches:**

| Comparison | Efficiency Gain (Synthetic) | Efficiency Gain (Full Codebase) |
|------------|-----------------------------|---------------------------------|
| MCP vs Manual Grep+Read | ~20.4x more efficient | **40-43x more efficient** |
| MCP vs Optimal Drag-and-Drop | ~1.7x more efficient | ~1.2-1.3x more efficient |

> **Note:** Full codebase tests (249 files) show higher efficiency gains than synthetic fixtures (25 files) because larger codebases have more irrelevant content that grep matches.

### Why MCP Wins

1. **No expertise required** - AI discovers relevant code automatically
2. **Focused results** - Only relevant portions returned (~7,800 tokens avg)
3. **Semantic understanding** - Finds conceptually related content
4. **Consistent performance** - Same efficiency regardless of codebase size
5. **Automatic deduplication** - 15-17% reduction from overlapping chunks

### The Hidden Cost of Alternatives

**Grep:** Would require reading 25-49 entire files per query. For a single question, that's 108K-192K tokens of context - often exceeding AI context limits entirely.

**Drag-and-Drop:** Requires the user to already understand the codebase structure. The "efficiency" comes at the cost of human expertise and time spent identifying files.

### Bottom Line

For AI assistants working with code, MCP enables asking questions about large codebases that would otherwise be impractical. The **40-43x efficiency gain** over grep means the difference between "context limit exceeded" and "here's your answer in milliseconds."

---

## Configuration Matrix Testing

We also tested 21 different configuration combinations to find optimal settings:

### Best Configurations (Synthetic Fixture - 25 files)

| Category | Best Config | Value |
|----------|-------------|-------|
| Lowest Latency | all-features | 18.8ms |
| Highest Precision@5 | default | 22% |
| Best Token Efficiency | all-features | 6,954 tokens |

### Alpha Parameter Analysis

| Alpha | Description | MCP vs Grep | Best For |
|-------|-------------|-------------|----------|
| 0.0 | Pure FTS/keyword | 37.9x | Exact matches |
| 0.5 | **Default (balanced hybrid)** | **43x** | General queries |
| 0.7 | Semantic-heavy | 40.9x | Conceptual queries |
| 1.0 | Pure semantic | 40.9x | Abstract concepts |

**Default:** `alpha=0.5` provides the best balance of efficiency and relevance (as of v1.3.4).

### Deduplication Effectiveness

| Config | Raw Results | After Dedup | Reduction |
|--------|-------------|-------------|-----------|
| default | 79 | 67 | 15% |
| alpha-0.0 | 80 | 68 | 15% |
| alpha-0.5 | 66 | 55 | **17%** |
| alpha-1.0 | 79 | 67 | 15% |

---

## Running the Tests

```bash
# Run config matrix tests (synthetic fixture, fast)
npm run test:configs

# Run full codebase tests (real project, slower)
npm run test:configs:full

# Run benchmark comparison test
npx vitest run tests/benchmarks/search-comparison.test.ts
```

Reports are generated in `tests/reports/`:
- `config-matrix-YYYY-MM-DD.md` - Configuration comparison
- `accuracy-comparison-YYYY-MM-DD.md` - MCP vs Grep vs D&D
